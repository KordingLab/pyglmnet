{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n==============================\nPoisson (Canonical) Distribution\n==============================\n\nThis is an example demonstrating ``Pyglmnet`` with\nthe canonical (exponential) link function for Poisson distributed targets.\n\nHere, we deal with the numerical instability of the exponential\nlink function by linearizing it above a certain threshold, ``eta``.\n\nThis canonical link can be used by specifying ``distr`` = ``'poisson'``.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's import useful libraries that we will use it later on\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Author: Pavan Ramkumar <pavan.ramkumar@gmail.com>\n# License: MIT\n\nimport numpy as np\nimport scipy.sparse as sps\nfrom scipy.special import expit\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Consider the negative log-likelihood loss function\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "\\begin{align}J = \\sum_i \\lambda_i - y_i \\log \\lambda_i\\end{align}\n\nInstead of the canonical link, we define \\lambda as follows:\n\n\\begin{align}\\lambda_i =\n    \\begin{cases}\n        \\exp(z_i), & \\text{if}\\ z_i \\leq \\eta \\\\\n        \\\\\n         \\exp(\\eta)z_i + (1-\\eta)\\exp(\\eta), & \\text{if}\\ z_i \\gt \\eta\n    \\end{cases}\\end{align}\n\nwhere\n\n\\begin{align}z_i = \\beta_0 + \\sum_j \\beta_j x_{ij}\\end{align}\n\nTaking gradients,\n\n\\begin{align}\\frac{\\partial J}{\\partial \\beta_j} = \\sum_i \\frac{\\partial J}{\\partial \\lambda_i} \\frac{\\partial \\lambda_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\beta_j}\\end{align}\n\n\n\\begin{align}\\frac{\\partial J}{\\partial \\beta_0} =\n    \\begin{cases}\n        \\sum_i \\Big(\\lambda_i - y_i\\Big), & \\text{if}\\ z_i \\leq \\eta \\\\\n        \\\\\n        \\exp(\\eta) \\sum_i \\Big(1 - \\frac{\\lambda_i}{y_i}\\Big), & \\text{if}\\ z_i \\gt \\eta\n    \\end{cases}\\end{align}\n\n\\begin{align}\\frac{\\partial J}{\\partial \\beta_j} =\n    \\begin{cases}\n        \\sum_i \\Big(\\lambda_i - y_i\\Big)x_{ij}, & \\text{if}\\ z_i \\leq \\eta \\\\\n        \\\\\n        \\exp(\\eta) \\sum_i \\Big(1 - \\frac{\\lambda_i}{y_i}\\Big)x_{ij}, & \\text{if}\\ z_i \\gt \\eta\n    \\end{cases}\\end{align}\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's write a piece of code to visualize the difference\nbetween exponential and linearized exponential.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "z = np.linspace(0., 10., 100)\n\neta = 4.0\nqu = deepcopy(z)\nslope = np.exp(eta)\nintercept = (1 - eta) * slope\nqu[z > eta] = z[z > eta] * slope + intercept\nqu[z <= eta] = np.exp(z[z <= eta])\n\nplt.plot(z, qu, label='lineared exp')\nplt.plot(z, np.exp(z), label='exp')\nplt.ylim([0, 1000])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=1,\n           ncol=2, borderaxespad=0.)\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Import ``GLM`` class from ``pyglmnet``\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# import GLM model\nfrom pyglmnet import GLM\n\n# create regularization parameters for model\nreg_lambda = np.logspace(np.log(0.5), np.log(0.01), 10, base=np.exp(1))\nglm_poissonexp = GLM(distr='poisson', verbose=False, alpha=0.05,\n            max_iter=1000, learning_rate=2e-1, score_metric='pseudo_R2',\n            reg_lambda=reg_lambda, eta=4.0)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Dataset size\nn_samples, n_features = 10000, 100\n\n# baseline term\nbeta0 = np.random.normal(0.0, 1.0, 1)\n# sparse model terms\nbeta = sps.rand(n_features, 1, 0.1)\nbeta = np.array(beta.todense())\n\n# training data\nXr = np.random.normal(0.0, 1.0, [n_samples, n_features])\nyr = glm_poissonexp.simulate(beta0, beta, Xr)\n\n# testing data\nXt = np.random.normal(0.0, 1.0, [n_samples, n_features])\nyt = glm_poissonexp.simulate(beta0, beta, Xt)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Fit model to training data\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "scaler = StandardScaler().fit(Xr)\nglm_poissonexp.fit(scaler.transform(Xr),yr);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Use one model to predict\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "m = glm_poissonexp[-1]\nthis_model_param = m.fit_\nyrhat = m.predict(scaler.transform(Xr))\nythat = m.predict(scaler.transform(Xt))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Visualize predicted output\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "plt.plot(yt[:100], label='tr')\nplt.plot(ythat[:100], 'r', label='pr')\nplt.xlabel('samples')\nplt.ylabel('true and predicted outputs')\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=1,\n           ncol=2, borderaxespad=0.)\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Compute pseudo R2\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(m.score(Xt, yt))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.12", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}