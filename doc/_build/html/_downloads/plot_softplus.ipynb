{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n===============================\nPoisson (Softplus) Distribution\n===============================\n\nFor Poisson distributed target variables, the canonical link function is an\nexponential nonlinearity. However, the gradient of the loss function that uses\nthis nonlinearity is typically unstable.\n\nThis is an example demonstrating how pyglmnet\nworks with a Poisson distribution using the softplus nonlinearity.\n\nHere, for the ``distr = 'softplus'`` option, we use the\nsoftplus function: ``log(1+exp())``.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "First, let's import useful libraries that we will use it later on.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Author: Pavan Ramkumar <pavan.ramkumar@gmail.com>\n# License: MIT\n\nimport numpy as np\nimport scipy.sparse as sps\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Here are inputs that you can provide when you instantiate the `GLM` class.\nIf not provided, it will be set to the respective defaults\n\n- `distr`: str (`'softplus'` or `'poisson'` or `'gaussian'` or `'binomial'` or `'multinomial'`)\n    default: `'poisson'`\n- `alpha`: float (the weighting between L1 and L2 norm)\n    default: 0.05\n- `reg_lambda`: array (array of regularization parameters)\n    default: `np.logspace(np.log(0.5), np.log(0.01), 10, base=np.exp(1))`\n- `learning_rate`: float (learning rate for gradient descent)\n    default: 2e-1\n- `max_iter`: int (maximum iteration for the model)\n    default: 1000\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Import ``GLM`` class from ``pyglmnet``\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# import GLM model\nfrom pyglmnet import GLM\n\n# create regularization parameters for model\nreg_lambda = np.logspace(np.log(0.5), np.log(0.01), 10, base=np.exp(1))\nglm_poisson = GLM(distr='softplus', verbose=False, alpha=0.05,\n            max_iter=1000, learning_rate=2e-1,\n            reg_lambda=reg_lambda)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Simulate a dataset\n------------------\nThe ``GLM`` class has a very useful method called ``simulate()``.\n\nSince a canonical link function is already specified by the distribution\nparameters, or provided by the user, ``simulate()`` requires\nonly the independent variables ``X`` and the coefficients ``beta0``\nand ``beta``\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "n_samples, n_features = 10000, 100\n\n# coefficients\nbeta0 = np.random.normal(0.0, 1.0, 1)\nbeta = sps.rand(n_features, 1, 0.1)\nbeta = np.array(beta.todense())\n\n# training data\nXr = np.random.normal(0.0, 1.0, [n_samples, n_features])\nyr = glm_poisson.simulate(beta0, beta, Xr)\n\n# testing data\nXt = np.random.normal(0.0, 1.0, [n_samples, n_features])\nyt = glm_poisson.simulate(beta0, beta, Xt)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Fit the model\n^^^^^^^^^^^^^\nFitting the model is accomplished by a single GLM method called `fit()`.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "scaler = StandardScaler().fit(Xr)\nglm_poisson.fit(scaler.transform(Xr), yr)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Slicing the model object\n^^^^^^^^^^^^^^^^^^^^^^^^\nAlthough the model is fit to all values of reg_lambda specified by a regularization\npath, often we are only interested in further analysis for a particular value of\n``reg_lambda``. We can easily do this by slicing the object.\n\nFor instance ``model[0]`` returns an object identical to model but with ``.fit_``\nas a dictionary corresponding to the estimated coefficients for ``reg_lambda[0]``.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Visualize the fit coefficients\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe estimated coefficients are stored in an instance variable called ``.fit_``\nwhich is a list of dictionaries. Each dictionary corresponds to a\nparticular ``reg_lambda``\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "fit_param = glm_poisson[-1].fit_\nplt.plot(beta[:], 'bo', label ='true')\nplt.plot(fit_param['beta'][:], 'ro', label='estimated')\nplt.xlabel('samples')\nplt.ylabel('outputs')\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=1,\n           ncol=2, borderaxespad=0.)\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Make predictions based on fit model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe ``predict()`` method takes two parameters: a numpy 2d array of independent\nvariables and a dictionary of fit parameters. It returns a vector of\npredicted targets.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Predict targets from test set\nyrhat = glm_poisson[-1].predict(scaler.transform(Xr))\nythat = glm_poisson[-1].predict(scaler.transform(Xt))\n\nplt.plot(yt[:100], label='true')\nplt.plot(ythat[:100], 'r', label='predicted')\nplt.xlabel('samples')\nplt.ylabel('true and predicted outputs')\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=1,\n           ncol=2, borderaxespad=0.)\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Goodness of fit\n^^^^^^^^^^^^^^^\nThe GLM class provides two metrics to evaluate the goodness of fit: ``deviance``\nand ``pseudo_R2``. Both these metrics are implemented in the ``score()`` method.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Compute model deviance\nDr = glm_poisson[-1].score(Xr, yr)\nDt = glm_poisson[-1].score(Xt, yt)\nprint('Dr = %f' % Dr, 'Dt = %f' % Dt)\n\n# Compute pseudo_R2s\nglm_poisson.score_metric = 'pseudo_R2'\nR2r = glm_poisson[-1].score(Xr, yr)\nR2t = glm_poisson[-1].score(Xt, yt)\nprint('  R2r =  %f' % R2r, ' R2r = %f' % R2t)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.12", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}