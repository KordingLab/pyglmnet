

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; pyglmnet 1.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/gallery.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="pyglmnet 1.1 documentation" href="index.html"/>
        <link rel="next" title="Cheatsheet" href="cheatsheet.html"/>
        <link rel="prev" title="Getting Started" href="start.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> pyglmnet
          

          
          </a>

          
            
            
              <div class="version">
                1.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="start.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#glm-with-elastic-net-penalty">GLM with elastic net penalty</a></li>
<li class="toctree-l2"><a class="reference internal" href="#poisson-like-glm">Poisson-like GLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#poisson-log-likelihood">Poisson Log-likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="#elastic-net-penalty">Elastic net penalty</a></li>
<li class="toctree-l2"><a class="reference internal" href="#objective-function">Objective function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-descent">Gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hessian-terms">Hessian terms</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cyclical-coordinate-descent">Cyclical coordinate descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization-paths-and-warm-restarts">Regularization paths and warm restarts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cheatsheet.html">Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Examples Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="whats_new.html">Release notes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">pyglmnet</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Tutorial</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="_sources/tutorial.rst.txt" rel="nofollow"> View page source</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>This is a tutorial on elastic net regularized generalized linear models.
We will go through the math to setup the penalized negative log-likelihood
loss function and the coordinate descent algorithm for optimization.</p>
<p>Here are some other resources from a
<a class="reference external" href="https:github.com/pavanramkumar/pydata-chicago-2016">PyData 2016 talk</a>.</p>
<p>At present this tutorial does not cover Tikhonov regularization or group lasso,
but we look forward to adding more material shortly.</p>
<p><strong>Reference</strong>
Jerome Friedman, Trevor Hastie and Rob Tibshirani. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, Vol. 33(1), 1-22 <a class="reference external" href="https://core.ac.uk/download/files/153/6287975.pdf">[pdf]</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Pavan Ramkumar</span>
<span class="c1"># License: MIT</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
</pre></div>
</div>
<div class="section" id="glm-with-elastic-net-penalty">
<h2>GLM with elastic net penalty<a class="headerlink" href="#glm-with-elastic-net-penalty" title="Permalink to this headline">¶</a></h2>
<p>In the elastic net regularized generalized Linear Model (GLM), we
want to solve the following convex optimization problem.</p>
<div class="math notranslate nohighlight">
\[\min_{\beta_0, \beta} \frac{1}{N} \sum_{i = 1}^N \mathcal{L} (y_i, \beta_0 + \beta^T x_i)
+ \lambda [\frac{1}{2}(1 - \alpha)\| \beta \|_2^2 + \alpha \| \beta \|_1]\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L} (y_i, \beta_0 + \beta^T x_i)\)</span> is the negative log-likelihood of
observation <span class="math notranslate nohighlight">\(i\)</span>. We will go through the softplus link function case
and show how we optimize the cost function.</p>
</div>
<div class="section" id="poisson-like-glm">
<h2>Poisson-like GLM<a class="headerlink" href="#poisson-like-glm" title="Permalink to this headline">¶</a></h2>
<p>The <cite>pyglmnet</cite> implementation comes with <cite>gaussian</cite>, <cite>binomial</cite>, <cite>probit</cite>
<cite>gamma</cite>, <cite>poisson</cite> and <cite>softplus</cite> distributions, but for illustration,
we will walk you through <cite>softplus</cite>: a particular adaptation of the canonical
Poisson generalized linear model (GLM).</p>
<p>For the Poisson GLM, <span class="math notranslate nohighlight">\(\lambda_i\)</span> is the rate parameter of an
inhomogeneous linear-nonlinear Poisson (LNP) process with instantaneous
mean given by:</p>
<div class="math notranslate nohighlight">
\[\lambda_i = \exp(\beta_0 + \beta^T x_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i \in \mathcal{R}^{p \times 1}, i = \{1, 2, \dots, n\}\)</span> are
the observed independent variables (predictors),
<span class="math notranslate nohighlight">\(\beta_0 \in \mathcal{R}^{1 \times 1}\)</span>,
<span class="math notranslate nohighlight">\(\beta \in \mathcal{R}^{p \times 1}\)</span>
are linear coefficients. The rate parameter <span class="math notranslate nohighlight">\(\lambda_i\)</span> is also known as
the conditional intensity function, conditioned on <span class="math notranslate nohighlight">\((\beta_0, \beta)\)</span> and
<span class="math notranslate nohighlight">\(q(z) = \exp(z)\)</span> is the nonlinearity.</p>
<p>For numerical reasons, let’s adopt a stabilizing non-linearity, known as the
<cite>softplus</cite> or the smooth rectifier (see <a class="reference external" href="http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf">Dugas et al., 2001</a>),
that has been adopted by Jonathan Pillow’s and Liam Paninski’s groups for neural data
analysis.
See for instance <a class="reference external" href="http://www.nature.com/neuro/journal/v17/n10/abs/nn.3800.html">Park et al., 2014</a>.</p>
<div class="math notranslate nohighlight">
\[q(z) = \log(1+\exp(z))\]</div>
<p>The <cite>softplus</cite> prevents <span class="math notranslate nohighlight">\(\lambda\)</span> in the canonical inverse link function
from exploding when the argument to the exponent is large. In this
modification, the formulation is no longer an exact LNP, nor an exact GLM,
but <span class="math notranslate nohighlight">\(\mathcal{L}(\beta_0, \beta)\)</span> is still concave (convex) and we
can use gradient ascent (descent) to optimize it.</p>
<div class="math notranslate nohighlight">
\[\lambda_i = q(\beta_0 + \beta^T x_i) = \log(1 + \exp(\beta_0 +
\beta^T x_i))\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
   <span class="sd">&quot;&quot;&quot;The non-linearity.&quot;&quot;&quot;</span>
   <span class="n">qu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">lmb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta0</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
   <span class="sd">&quot;&quot;&quot;Conditional intensity function.&quot;&quot;&quot;</span>
   <span class="n">z</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
   <span class="n">l</span> <span class="o">=</span> <span class="n">qu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">l</span>
</pre></div>
</div>
</div>
<div class="section" id="poisson-log-likelihood">
<h2>Poisson Log-likelihood<a class="headerlink" href="#poisson-log-likelihood" title="Permalink to this headline">¶</a></h2>
<p>The likelihood of observing the spike count <span class="math notranslate nohighlight">\(y_i\)</span> under the Poisson
likelihood function with inhomogeneous rate <span class="math notranslate nohighlight">\(\lambda_i\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\prod_i P(y = y_i | X) = \prod_i \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!}\]</div>
<p>The log-likelihood is given by:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \sum_i \bigg\{y_i \log(\lambda_i) - \lambda_i
- \log(y_i!)\bigg\}\]</div>
<p>However, we are interested in maximizing the log-likelihood with respect to
<span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. Thus, we can drop the factorial term:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\beta_0, \beta) = \sum_i \bigg\{y_i \log(\lambda_i)
- \lambda_i\bigg\}\]</div>
</div>
<div class="section" id="elastic-net-penalty">
<h2>Elastic net penalty<a class="headerlink" href="#elastic-net-penalty" title="Permalink to this headline">¶</a></h2>
<p>For large models we need to penalize the log likelihood term in order to
prevent overfitting. The elastic net penalty is given by:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}_\alpha(\beta) = (1-\alpha)\frac{1}{2} \|\beta\|^2_{\mathcal{l}_2} + \alpha\|\beta\|_{\mathcal{l}_1}\]</div>
<p>The elastic net interpolates between two extremes.
When <span class="math notranslate nohighlight">\(\alpha = 0\)</span> the penalized model is known as ridge regression and
when <span class="math notranslate nohighlight">\(\alpha = 1\)</span> it is known as LASSO. Note that we do not penalize the
baseline term <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">penalty</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
   <span class="sd">&quot;&quot;&quot;the penalty term&quot;&quot;&quot;</span>
   <span class="n">P</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> \
         <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">P</span>
</pre></div>
</div>
</div>
<div class="section" id="objective-function">
<h2>Objective function<a class="headerlink" href="#objective-function" title="Permalink to this headline">¶</a></h2>
<p>We minimize the objective function:</p>
<div class="math notranslate nohighlight">
\[J(\beta_0, \beta) = -\mathcal{L}(\beta_0, \beta) + \lambda \mathcal{P}_\alpha(\beta)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L}(\beta_0, \beta)\)</span> is the Poisson log-likelihood and
<span class="math notranslate nohighlight">\(\mathcal{P}_\alpha(\beta)\)</span> is the elastic net penalty term and
<span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\alpha\)</span> are regularization parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">beta0</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
   <span class="sd">&quot;&quot;&quot;Define the objective function for elastic net.&quot;&quot;&quot;</span>
   <span class="n">L</span> <span class="o">=</span> <span class="n">logL</span><span class="p">(</span><span class="n">beta0</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
   <span class="n">P</span> <span class="o">=</span> <span class="n">penalty</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
   <span class="n">J</span> <span class="o">=</span> <span class="o">-</span><span class="n">L</span> <span class="o">+</span> <span class="n">reg_lambda</span> <span class="o">*</span> <span class="n">P</span>
   <span class="k">return</span> <span class="n">J</span>
</pre></div>
</div>
</div>
<div class="section" id="gradient-descent">
<h2>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>To calculate the gradients of the cost function with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span> and
<span class="math notranslate nohighlight">\(\beta\)</span>, let’s plug in the definitions for the log likelihood and penalty terms from above.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
    J(\beta_0, \beta) &amp;= \sum_i \bigg\{ \log(1 + \exp(\beta_0 + \beta^T x_i))\\
      &amp; - y_i \log(\log(1 + \exp(\beta_0 + \beta^T x_i)))\bigg\}\\
      &amp; + \lambda(1 - \alpha)\frac{1}{2} \|\beta\|^2_{\mathcal{l_2}}
      + \lambda\alpha\|\beta\|_{\mathcal{l_1}}
\end{eqnarray}\end{split}\]</div>
<p>Since we will apply coordinate descent, let’s rewrite this cost in terms of each
scalar parameter <span class="math notranslate nohighlight">\(\beta_j\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
    J(\beta_0, \beta) &amp;= \sum_i \bigg\{ \log(1 + \exp(\beta_0 + \sum_j \beta_j x_{ij}))
    &amp; - y_i \log(\log(1 + \exp(\beta_0 + \sum_j \beta_j x_{ij})))\bigg\}\\
    &amp; + \lambda(1-\alpha)\frac{1}{2} \sum_j \beta_j^2 + \lambda\alpha\sum_j \mid\beta_j\mid
\end{eqnarray}\end{split}\]</div>
<p>Let’s take the derivatives of some big expressions using chain rule.
Define <span class="math notranslate nohighlight">\(z_i = \beta_0 + \sum_j \beta_j x_{ij}\)</span>.</p>
<p>For the nonlinearity in the first term <span class="math notranslate nohighlight">\(y = q(z) = \log(1+e^{z(\theta)})\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\frac{\partial y}{\partial \theta} &amp;= \frac{\partial q}{\partial z}\frac{\partial z}{\partial \theta}\\
&amp; = \frac{e^z}{1+e^z}\frac{\partial z}{\partial \theta}\\
&amp; = \sigma(z)\frac{\partial z}{\partial \theta}
\end{eqnarray}\end{split}\]</div>
<p>For the nonlinearity in the second term <span class="math notranslate nohighlight">\(y = \log(q(z)) = \log(\log(1+e^{z(\theta)}))\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\frac{\partial y}{\partial \theta} &amp; = \frac{1}{q(z)}\frac{\partial q}{\partial z}\frac{\partial z}{\partial \theta}\\
&amp; = \frac{\sigma(z)}{q(z)}\frac{\partial z}{\partial \theta}
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\dot q(z)\)</span> happens to be be the sigmoid function</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{e^z}{1+e^z}.\]</div>
<p>Putting it all together we have:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J}{\partial \beta_0} = \sum_i \sigma(z_i) - \sum_i y_i\frac{\sigma(z_i)}{q(z_i)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J}{\partial \beta_j} = \sum_i \sigma(z_i) x_{ij} - \sum_i y_i \frac{\sigma(z_i)}{q(z_i)}x_{ij}
+ \lambda(1-\alpha)\beta_j + \lambda\alpha \text{sgn}(\beta_j)\]</div>
<p>Let’s define these gradients:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grad_L2loss</span><span class="p">(</span><span class="n">beta0</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
   <span class="n">z</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
   <span class="n">s</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
   <span class="n">q</span> <span class="o">=</span> <span class="n">qu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
   <span class="n">grad_beta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">s</span> <span class="o">/</span> <span class="n">q</span><span class="p">)</span>
     <span class="n">grad_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span> <span class="o">-</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">s</span> <span class="o">/</span> <span class="n">q</span><span class="p">),</span> <span class="n">X</span><span class="p">))</span> <span class="o">+</span> \
     <span class="n">reg_lambda</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">beta</span>
     <span class="k">return</span> <span class="n">grad_beta0</span><span class="p">,</span> <span class="n">grad_beta</span>
</pre></div>
</div>
<p>Note that this is all we need for a classic batch gradient descent implementation,
implemented in the <code class="docutils literal notranslate"><span class="pre">'batch-gradient'</span></code> solver.</p>
<p>However, let’s also derive the Hessian terms that will be useful for second-order
optimization methods implemented in the <code class="docutils literal notranslate"><span class="pre">'cdfast'</span></code> solver.</p>
</div>
<div class="section" id="hessian-terms">
<h2>Hessian terms<a class="headerlink" href="#hessian-terms" title="Permalink to this headline">¶</a></h2>
<p>Second-order derivatives can accelerate convergence to local minima by providing
optimal step sizes. However, they are expensive to compute.</p>
<p>This is where coordinate descent shines. Since we update only one parameter
<span class="math notranslate nohighlight">\(\beta_j\)</span> per step, we can simply use the <span class="math notranslate nohighlight">\(j^{th}\)</span> diagonal term in
the Hessian matrix to perform an approximate Newton update as:</p>
<div class="math notranslate nohighlight">
\[\beta_j^{t+1} = \beta_j^{t} - \bigg\{\frac{\partial^2 J}{\partial \beta_j^2}\bigg\}^{-1} \frac{\partial J}{\partial \beta_j}\]</div>
<p>Let’s use calculus again to compute these diagonal terms. Recall that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\dot q(z) &amp; = \sigma(z)\\
\dot\sigma(z) &amp; = \sigma(z)(1-\sigma(z))
\end{eqnarray}\end{split}\]</div>
<p>Using these, and applying the product rule</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial z}\bigg\{ \frac{\sigma(z)}{q(z)} \bigg\} = \frac{\sigma(z)(1-\sigma(z))}{q(z)} - \frac{\sigma(z)}{q(z)^2}\]</div>
<p>Plugging all these in, we get</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 J}{\partial \beta_0^2} = \sum_i \sigma(z_i)(1 - \sigma(z_i)) - \sum_i y_i \bigg\{ \frac{\sigma(z_i) (1 - \sigma(z_i))}{q(z_i)} - \frac{\sigma(z_i)}{q(z_i)^2} \bigg\}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\frac{\partial^2 J}{\partial \beta_j^2} &amp; = \sum_i \sigma(z_i)(1 - \sigma(z_i)) x_{ij}^2 \\
&amp; - \sum_i y_i \bigg\{ \frac{\sigma(z_i) (1 - \sigma(z_i))}{q(z_i)} \\
&amp; - \frac{\sigma(z_i)}{q(z_i)^2} \bigg\}x_{ij}^2 + \lambda(1-\alpha)
\end{eqnarray}\end{split}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hessian_loss</span><span class="p">(</span><span class="n">beta0</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">qu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">grad_s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">s</span><span class="p">)</span>
    <span class="n">grad_s_by_q</span> <span class="o">=</span> <span class="n">grad_s</span><span class="o">/</span><span class="n">q</span> <span class="o">-</span> <span class="n">s</span><span class="o">/</span><span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">q</span><span class="p">)</span>
    <span class="n">hess_beta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">grad_s_by_q</span><span class="p">)</span>
    <span class="n">hess_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">grad_s</span><span class="p">),</span> <span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">grad_s_by_q</span><span class="p">),</span> <span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="p">))</span>\
                <span class="o">+</span> <span class="n">reg_lambda</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hess_beta0</span><span class="p">,</span> <span class="n">hess_beta</span>
</pre></div>
</div>
<p>Also see the <a class="reference external" href="http://glm-tools.github.io/pyglmnet/cheatsheet.html">cheatsheet</a> for the calculus required
to derive gradients and Hessians for other distributions.</p>
</div>
<div class="section" id="cyclical-coordinate-descent">
<h2>Cyclical coordinate descent<a class="headerlink" href="#cyclical-coordinate-descent" title="Permalink to this headline">¶</a></h2>
<p><strong>Parameter update step</strong></p>
<p>In cylical coordinate descent with elastic net, we store an active set
<span class="math notranslate nohighlight">\(\mathcal{K}\)</span> of parameter indices that we update. Since the <span class="math notranslate nohighlight">\(\mathcal{l}_1\)</span>
terms <span class="math notranslate nohighlight">\(|\beta_j|\)</span> are not differentiable at zero, we use the gradient without
the <span class="math notranslate nohighlight">\(\lambda\alpha \text{sgn}(\beta_j)\)</span> term to update <span class="math notranslate nohighlight">\(\beta_j\)</span>.
Let’s call these gradient terms <span class="math notranslate nohighlight">\(\tilde{g}_k\)</span>.</p>
<p>We start by initializing <span class="math notranslate nohighlight">\(\mathcal{K}\)</span> to contain all parameter indices.
Let’s say only the <span class="math notranslate nohighlight">\(k^{th}\)</span> parameter is updated at time step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
    \beta_k^{t} &amp; = \beta_k^{t-1} - (h_k^{t-1})^{-1} \tilde{g}_k^{t-1} \\
    \beta_j^{t} &amp; = \beta_j^{t-1}, \forall j \neq k
\end{eqnarray}\end{split}\]</div>
<p>Next we apply a soft thresholding step for <span class="math notranslate nohighlight">\(k \neq 0\)</span> after every update iteration, as follows.
<span class="math notranslate nohighlight">\(\beta_k^{t} = \mathcal{S}_{\lambda\alpha}(\beta_k^{t})\)</span></p>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}S_\lambda(x) =
\begin{cases}
0 &amp; \text{if} &amp; |x| \leq \lambda\\
\text{sgn}(x)||x|-\lambda| &amp; \text{if} &amp; |x| &gt; \lambda
\end{cases}\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(\beta_k^{t}\)</span> has been zero-ed out, we remove <span class="math notranslate nohighlight">\(k\)</span> from the active set.</p>
<div class="math notranslate nohighlight">
\[\mathcal{K} = \mathcal{K} \setminus \left\{k\right\}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prox</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Proximal operator.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">l</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">l</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Caching the z update step</strong></p>
<p>Next we want to update <span class="math notranslate nohighlight">\(\beta_{k+1}\)</span> at the next time step <span class="math notranslate nohighlight">\(t+1\)</span>.
For this we need the gradient and Hessian terms, <span class="math notranslate nohighlight">\(\tilde{g}_{k+1}\)</span> and
<span class="math notranslate nohighlight">\(h_{k+1}\)</span>. If we update them instead of recalculating them, we can save on
a lot of multiplications and additions. This is possible because we only update
one parameter at a time. Let’s calculate how to make these updates.</p>
<div class="math notranslate nohighlight">
\[z_i^{t} = z_i^{t-1} - \beta_k^{t-1}x_{ik} + \beta_k^{t}x_{ik}\]</div>
<div class="math notranslate nohighlight">
\[z_i^{t} = z_i^{t-1} - (h_k^{t-1})^{-1} \tilde{g}_k^{t-1}x_{ik}\]</div>
<p><strong>Gradient update</strong></p>
<p>If <span class="math notranslate nohighlight">\(k = 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\tilde{g}_{k+1}^t = \sum_i \sigma(z_i^t) - \sum_i y_i \frac{\sigma(z_i^t)}{q(z_i^t)}\]</div>
<p>If <span class="math notranslate nohighlight">\(k &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{eqnarray}
    \tilde{g}_{k+1}^t &amp; = \sum_i \sigma(z_i^t) x_{i,k+1} - \sum_i y_i \frac{\sigma(z_i^t)}{q(z_i^t)} x_{i,k+1}
      &amp; + \lambda(1-\alpha)\beta_{k+1}^t
\end{eqnarray}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grad_loss_k</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">beta_k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">rl</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient update for a single coordinate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">qu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">gk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">s</span><span class="o">/</span><span class="n">q</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="o">*</span><span class="n">xk</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">s</span><span class="o">/</span><span class="n">q</span><span class="o">*</span><span class="n">xk</span><span class="p">)</span> <span class="o">+</span> <span class="n">rl</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">beta_k</span>
    <span class="k">return</span> <span class="n">gk</span>
</pre></div>
</div>
<p><strong>Hessian update</strong></p>
<p>If <span class="math notranslate nohighlight">\(k = 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}h_{k+1}^t &amp; = \sum_i \sigma(z_i^t)(1 - \sigma(z_i^t)) \\
&amp; - \sum_i y_i \bigg\{ \frac{\sigma(z_i^t) (1 - \sigma(z_i^t))}{q(z_i^t)} - \frac{\sigma(z_i^t)}{q(z_i^t)^2} \bigg\}\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(k &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
h_{k+1}^t &amp; = \sum_i \sigma(z_i^t)(1 - \sigma(z_i^t)) x_{i,k+1}^2 \\
&amp; - \sum_i y_i \bigg\{ \frac{\sigma(z_i^t) (1 - \sigma(z_i^t))}{q(z_i^t)}
&amp; - \frac{\sigma(z_i^t)}{q(z_i^t)^2} \bigg\}x_{i,k+1}^2 + \lambda(1-\alpha)
\end{eqnarray}\end{split}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hess_loss_k</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">rl</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Hessian update for a single coordinate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">qu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">grad_s</span> <span class="o">=</span> <span class="n">s</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">s</span><span class="p">)</span>
    <span class="n">grad_s_by_q</span> <span class="o">=</span> <span class="n">grad_s</span><span class="o">/</span><span class="n">q</span> <span class="o">-</span> <span class="n">s</span><span class="o">/</span><span class="p">(</span><span class="n">q</span><span class="o">*</span><span class="n">q</span><span class="p">)</span>
    <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">hk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">grad_s_by_q</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">hk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_s</span><span class="o">*</span><span class="n">xk</span><span class="o">*</span><span class="n">xk</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">grad_s_by_q</span><span class="o">*</span><span class="n">xk</span><span class="o">*</span><span class="n">xk</span><span class="p">)</span> <span class="o">+</span> <span class="n">rl</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hk</span>
</pre></div>
</div>
</div>
<div class="section" id="regularization-paths-and-warm-restarts">
<h2>Regularization paths and warm restarts<a class="headerlink" href="#regularization-paths-and-warm-restarts" title="Permalink to this headline">¶</a></h2>
<p>We often find the optimal regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span> through cross-validation.
In practice we therefore often fit the model several times over a range of <span class="math notranslate nohighlight">\(\lambda\)</span>’s
<span class="math notranslate nohighlight">\(\{ \lambda_{max} \geq \dots \geq \lambda_0\}\)</span>.</p>
<p>Instead of re-fitting the model each time, we can solve the problem for the
most-regularized model (<span class="math notranslate nohighlight">\(\lambda_{max}\)</span>) and then initialize the subsequent
model with this solution. The path that each parameter takes through the range of
regularization parameters is known as the regularization path, and the trick of
initializing each model with the previous model’s solution is known as a warm restart.</p>
<p>In practice, this significantly speeds up convergence.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cheatsheet.html" class="btn btn-neutral float-right" title="Cheatsheet" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="start.html" class="btn btn-neutral" title="Getting Started" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016-2019, Pavan Ramkumar.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/language_data.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>